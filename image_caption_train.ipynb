{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. generate data\n",
    "    Loads vocab\n",
    "    Loads image features\n",
    "    Provide data for training\n",
    "    \n",
    "2. Builds image caption model\n",
    "3. Training \n",
    "4. Evaluation\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow import gfile\n",
    "from tensorflow import logging\n",
    "import pprint\n",
    "# import cPickle # for python 2\n",
    "import _pickle as cPickle  # for python 3\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "input_description_file = \"./data/results_20130124.token\"\n",
    "input_img_feature_dir = \"./data/feature_extraction_inception_v3\"\n",
    "input_vocab_file = \"./data/vocab.txt\"\n",
    "output_dir = \"./data/local_run\"\n",
    "checkpoint_dir = \"./data/local_run\"\n",
    "\n",
    "if not gfile.Exists(output_dir):\n",
    "    gfile.MakeDirs(output_dir)\n",
    "\n",
    "# hyper parameters\n",
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_vocab_word_threshold=3,\n",
    "        # LSTM\n",
    "        num_embedding_nodes=32,\n",
    "        num_timesteps=10,\n",
    "        num_lstm_nodes=[64, 64],\n",
    "        num_lstm_layers=2,\n",
    "        # fc\n",
    "        num_fc_nodes=32,\n",
    "        # train\n",
    "        batch_size=50,\n",
    "        cell_type='lstm',\n",
    "        # clip gradient\n",
    "        clip_lstm_grads=1.0,\n",
    "        learning_rate=0.001,\n",
    "        keep_prob=0.8,\n",
    "        # save file\n",
    "        log_frequent=10,\n",
    "        save_frequent=100,\n",
    "    )\n",
    "\n",
    "hps = get_default_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 10875\n",
      "[1494, 389, 1, 0]\n",
      "'the young on toddler'\n"
     ]
    }
   ],
   "source": [
    "class Vocab(object):\n",
    "    \"\"\"Loads vocab table\"\"\"\n",
    "    def __init__(self, filename, word_num_threshold):\n",
    "        self._id_to_word = {}\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._eos = -1\n",
    "        self._word_num_threshold = word_num_threshold\n",
    "        self._read_dict(filename)\n",
    "\n",
    "    def _read_dict(self, filename):\n",
    "        with gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, occurence = line.strip('\\r\\n').split('\\t')\n",
    "            occurence = int(occurence)\n",
    "            if word != '<UNK>' and occurence < self._word_num_threshold:\n",
    "                continue\n",
    "            idx = len(self._id_to_word)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '.':\n",
    "                self._eos = idx\n",
    "            if idx in self._id_to_word or word in self._word_to_id:\n",
    "                raise Exception('duplicate words in vocab file')\n",
    "            self._word_to_id[word] = idx\n",
    "            self._id_to_word[idx] = word\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self.unk)\n",
    "\n",
    "    def id_to_word(self, cur_id):\n",
    "        return self._id_to_word.get(cur_id, '<UNK>')\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split(' ')]\n",
    "        return word_ids\n",
    "\n",
    "    def decode(self, sentence_id):\n",
    "        words = [self.id_to_word(word_id) for word_id in sentence_id]\n",
    "        return ' '.join(words)\n",
    "    \n",
    "# testing\n",
    "vocab = Vocab(input_vocab_file, hps.num_vocab_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "logging.info(\"vocab_size: %d\" % vocab_size)\n",
    "\n",
    "pprint.pprint(vocab.encode(\"I have a dream\"))\n",
    "pprint.pprint(vocab.decode([5,23,6,352]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:num of all images: 31783\n",
      "['1000092795.jpg',\n",
      " '10002456.jpg',\n",
      " '1000268201.jpg',\n",
      " '1000344755.jpg',\n",
      " '1000366164.jpg']\n",
      "['A man in jeans is reclining on a green metal bench along a busy sidewalk and '\n",
      " 'crowded street .',\n",
      " 'A white male with a blue sweater and gray pants laying on a sidewalk bench .',\n",
      " 'A man in a blue shirt and gray pants is sleeping on a sidewalk bench .',\n",
      " 'A person is sleeping on a bench , next to cars .',\n",
      " 'A man sleeping on a bench in a city area .']\n",
      "INFO:tensorflow:num of all images: 31783\n",
      "['1000092795.jpg',\n",
      " '10002456.jpg',\n",
      " '1000268201.jpg',\n",
      " '1000344755.jpg',\n",
      " '1000366164.jpg']\n",
      "[[3, 9, 4, 132, 8, 3532, 6, 1, 48, 337, 146, 139, 1, 244, 93, 7, 380, 36, 2],\n",
      " [3, 20, 179, 11, 1, 26, 284, 7, 120, 128, 297, 6, 1, 93, 146, 2],\n",
      " [3, 9, 4, 1, 26, 21, 7, 120, 128, 8, 340, 6, 1, 93, 146, 2],\n",
      " [3, 63, 8, 340, 6, 1, 146, 12, 70, 15, 518, 2],\n",
      " [3, 9, 340, 6, 1, 146, 4, 1, 112, 171, 2]]\n"
     ]
    }
   ],
   "source": [
    "def parse_token_file(token_file):\n",
    "    \"\"\"Parses image description file into a dict with \n",
    "    key: img_names, value: a list of descriptions\"\"\"\n",
    "    img_name_to_tokens = {}\n",
    "    with gfile.GFile(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        img_id, description = line.strip('\\r\\n').split('\\t')\n",
    "        img_name, _ = img_id.split('#')\n",
    "        img_name_to_tokens.setdefault(img_name, [])\n",
    "        img_name_to_tokens[img_name].append(description)\n",
    "    return img_name_to_tokens\n",
    "\n",
    "def convert_token_to_id(img_name_to_tokens, vocab):\n",
    "    \"\"\"Converts each description of imgs to a list of id. \"\"\"\n",
    "    img_name_to_token_ids = {}\n",
    "    for img_name in img_name_to_tokens:\n",
    "        img_name_to_token_ids.setdefault(img_name, [])\n",
    "        descriptions = img_name_to_tokens[img_name]\n",
    "        for description in descriptions:\n",
    "            token_ids = vocab.encode(description)\n",
    "            img_name_to_token_ids[img_name].append(token_ids)\n",
    "    return img_name_to_token_ids\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "img_name_to_tokens = parse_token_file(input_description_file)\n",
    "img_name_to_token_ids = convert_token_to_id(img_name_to_tokens, vocab)\n",
    "\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_tokens))\n",
    "pprint.pprint(list(img_name_to_tokens.keys())[0:5])\n",
    "pprint.pprint(img_name_to_tokens['2778832101.jpg'])\n",
    "\n",
    "logging.info(\"num of all images: %d\" % len(img_name_to_token_ids))\n",
    "pprint.pprint(list(img_name_to_token_ids.keys())[0:5])\n",
    "pprint.pprint(img_name_to_token_ids['2778832101.jpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/feature_extraction_inception_v3/image_features-22.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-0.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-1.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-10.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-11.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-12.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-13.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-14.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-15.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-16.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-17.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-18.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-19.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-2.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-20.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-21.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-23.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-24.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-25.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-26.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-27.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-28.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-29.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-3.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-30.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-31.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-4.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-5.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-6.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-7.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-8.pickle',\n",
      " './data/feature_extraction_inception_v3/image_features-9.pickle']\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-22.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-0.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-1.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-10.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-11.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-12.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-13.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-14.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-15.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-16.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-17.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-18.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-19.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-2.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-20.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-21.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-23.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-24.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-25.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-26.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-27.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-28.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-29.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-3.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-30.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-31.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-4.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-5.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-6.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-7.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-8.pickle\n",
      "INFO:tensorflow:loading ./data/feature_extraction_inception_v3/image_features-9.pickle\n",
      "(31783, 2048)\n",
      "(31783,)\n",
      "INFO:tensorflow:img_feature_dim: 2048\n",
      "INFO:tensorflow:caption_data_size: 31783\n",
      "array([[ 0.12849414,  0.21631332,  0.19857083, ...,  0.25199878,\n",
      "         0.12173364,  0.07676762],\n",
      "       [ 0.30037332,  0.35706463,  0.23757873, ...,  0.28497294,\n",
      "         0.11285494,  0.02049274],\n",
      "       [ 1.40676665,  0.37377936,  0.26699796, ...,  0.24598999,\n",
      "         0.06566387,  0.30181745],\n",
      "       [ 0.2011404 ,  0.23911543,  0.1856163 , ...,  0.60713077,\n",
      "         0.40798631,  0.05803344],\n",
      "       [ 0.1159963 ,  0.29939425,  0.15878586, ...,  0.15415573,\n",
      "         0.1383785 ,  0.14591825]], dtype=float32)\n",
      "array([[ 256,   63,   32,    6,    1,  145,   12,   42,    1,  161],\n",
      "       [  45,   88,  224,   51,    1,  482,  474,   25, 2618,    6],\n",
      "       [  16, 1813,  734,    0,    1,    0,    0,  241,  143,    4],\n",
      "       [   3,   31,    8,   29,    6,    1,   73,  182,   11,   47],\n",
      "       [   3,   13,    4,   27,   84,  103,    8,   32,   70,   15]])\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "array(['3748481499.jpg', '5404950935.jpg', '4583731865.jpg',\n",
      "       '3706020467.jpg', '4671295213.jpg'],\n",
      "      dtype='<U14')\n"
     ]
    }
   ],
   "source": [
    "class ImageCaptionData(object):\n",
    "    \"\"\"Provide data for this model\"\"\"\n",
    "    def __init__(self,\n",
    "                 img_name_to_token_ids,\n",
    "                 img_feature_dir,\n",
    "                 num_timesteps,\n",
    "                 vocab,\n",
    "                 deterministic = False):\n",
    "        self._vocab = vocab\n",
    "        \n",
    "        self._img_name_to_token_ids = img_name_to_token_ids\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._indicator = 0\n",
    "        self._deterministic = deterministic\n",
    "        self._img_feature_filenames = []\n",
    "        self._img_feature_data = []\n",
    "        \n",
    "        self._all_img_feature_filepaths = []\n",
    "        for filename in gfile.ListDirectory(img_feature_dir):\n",
    "            self._all_img_feature_filepaths.append(\n",
    "                os.path.join(img_feature_dir, filename))\n",
    "        pprint.pprint(self._all_img_feature_filepaths)\n",
    "        \n",
    "        self._load_img_feature_pickle()\n",
    "        \n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "\n",
    "\n",
    "    def _load_img_feature_pickle(self):\n",
    "        \"\"\"Load img features data form pickle files\"\"\"\n",
    "        for filepath in self._all_img_feature_filepaths:\n",
    "            logging.info(\"loading %s\" % filepath)\n",
    "            with gfile.GFile(filepath, 'rb') as f:\n",
    "                filenames, features = cPickle.load(f, encoding='latin1')   # !!!\n",
    "                self._img_feature_filenames += filenames # merge\n",
    "                self._img_feature_data.append(features) # append\n",
    "        \n",
    "        # [(1000,1,1,2048), (1000,1,1,2048)] -> (2000,1,1,2048)\n",
    "        self._img_feature_data = np.vstack(self._img_feature_data)\n",
    "        \n",
    "        origin_shape = self._img_feature_data.shape\n",
    "         # (2000,1,1,2048)->(2000,2048)\n",
    "        self._img_feature_data = np.reshape( \n",
    "            self._img_feature_data, (origin_shape[0], origin_shape[3]))\n",
    "        self._img_feature_filenames = np.asarray(self._img_feature_filenames)\n",
    "        \n",
    "        print(self._img_feature_data.shape)\n",
    "        print(self._img_feature_filenames.shape)\n",
    "        if not self._deterministic:\n",
    "            self._random_shuffle()\n",
    "\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._img_feature_filenames)\n",
    "\n",
    "    def img_feature_size(self):\n",
    "        return self._img_feature_data.shape[1]\n",
    "\n",
    "    def _random_shuffle(self):\n",
    "        \"\"\"Shuffle data randomly\"\"\"\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._img_feature_filenames = self._img_feature_filenames[p]\n",
    "        self._img_feature_data = self._img_feature_data[p]\n",
    "\n",
    "    def _img_desc(self, filenames):\n",
    "        \"\"\"Get dexcription for filenames in  batch\"\"\"\n",
    "        batch_sentence_ids = []\n",
    "        batch_weights = []\n",
    "        \n",
    "        for filename in filenames:\n",
    "            token_ids_set = self._img_name_to_token_ids[filename]\n",
    "            # chosen_token_ids = random.choice(token_ids_set)\n",
    "            chosen_token_ids = token_ids_set[0]\n",
    "            chosen_token_length = len(chosen_token_ids)\n",
    "\n",
    "            weight = [1 for i in range(chosen_token_length)]\n",
    "            # 截断\n",
    "            if chosen_token_length >= self._num_timesteps:\n",
    "                chosen_token_ids = chosen_token_ids[0:self._num_timesteps]\n",
    "                weight = weight[0:self._num_timesteps]\n",
    "            else:# 填充\n",
    "                remaining_length = self._num_timesteps - chosen_token_length\n",
    "                chosen_token_ids += [self._vocab.eos for i in range(remaining_length)]\n",
    "                weight += [0 for i in range(remaining_length)]\n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_weights.append(weight)\n",
    "        # conver tot numpy arrays\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_weights = np.asarray(batch_weights)\n",
    "        \n",
    "        return batch_sentence_ids, batch_weights\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        if end_indicator > self.size():\n",
    "            if not self._deterministic:\n",
    "                self._random_shuffle()\n",
    "            self._indicator = 0  # reset\n",
    "            end_indicator = self._indicator + batch_size # reset\n",
    "        assert end_indicator <= self.size()\n",
    "\n",
    "        batch_img_features = self._img_feature_data[self._indicator: end_indicator]\n",
    "        batch_img_names = self._img_feature_filenames[self._indicator: end_indicator]\n",
    "        \n",
    "        # batch_sentence_ids:[100,34,23,1,0,0,0]->batch_weights:[1,1,1,1,0,0,0]\n",
    "        # '1' represents calculating the wieights, '0' no calculating \n",
    "        batch_sentence_ids, batch_weights = self._img_desc(batch_img_names)\n",
    "\n",
    "        self._indicator = end_indicator\n",
    "        return batch_img_features, batch_sentence_ids, batch_weights, batch_img_names\n",
    "\n",
    "\n",
    "caption_data = ImageCaptionData(img_name_to_token_ids, \n",
    "                                input_img_feature_dir,\n",
    "                                hps.num_timesteps, vocab)\n",
    "img_feature_dim = caption_data.img_feature_size()\n",
    "caption_data_size = caption_data.size()\n",
    "logging.info(\"img_feature_dim: %d\" % img_feature_dim)\n",
    "logging.info(\"caption_data_size: %d\" % caption_data_size)\n",
    "\n",
    "batch_img_features, batch_sentence_ids, batch_weights, batch_img_names = caption_data.next_batch(5)\n",
    "pprint.pprint(batch_img_features)\n",
    "pprint.pprint(batch_sentence_ids)\n",
    "pprint.pprint(batch_weights)\n",
    "pprint.pprint(batch_img_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-f591b0bc2aa5>:40: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "INFO:tensorflow:variable name: embedding/embeddings:0\n",
      "INFO:tensorflow:variable name: image_feature_embed/dense/kernel:0\n",
      "INFO:tensorflow:variable name: image_feature_embed/dense/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/logits/kernel:0\n",
      "INFO:tensorflow:variable name: fc/logits/bias:0\n",
      "INFO:tensorflow:Summary name embedding/embeddings:0_grad is illegal; using embedding/embeddings_0_grad instead.\n",
      "INFO:tensorflow:Summary name image_feature_embed/dense/kernel:0_grad is illegal; using image_feature_embed/dense/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name image_feature_embed/dense/bias:0_grad is illegal; using image_feature_embed/dense/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/kernel:0_grad is illegal; using fc/fc1/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/bias:0_grad is illegal; using fc/fc1/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/logits/kernel:0_grad is illegal; using fc/logits/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/logits/bias:0_grad is illegal; using fc/logits/bias_0_grad instead.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Construct the graph\n",
    "\"\"\"\n",
    "def create_rnn_cell(hidden_dim, cell_type):\n",
    "    \"\"\"Choose between lstm & gru\"\"\"\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)\n",
    "    elif cell_type == 'gru':\n",
    "        return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "    else:\n",
    "        raise Exception(\"%s has not been supported\" % cell_type)\n",
    "\n",
    "        \n",
    "def dropout(cell, keep_prob):\n",
    "    \"\"\"drop out operation for RNN layer, not suitable for fc\"\"\"\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "\n",
    "\n",
    "def get_train_model(hps, vocab_size, img_feature_dim):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "\n",
    "    img_feature  = tf.placeholder(tf.float32, (batch_size, img_feature_dim))\n",
    "    sentence = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    mask = tf.placeholder(tf.float32, (batch_size, num_timesteps))\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    global_step = tf.Variable(tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "\n",
    "    # Sets up the embedding layer.\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding', initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embeddings',\n",
    "            [vocab_size, hps.num_embedding_nodes],\n",
    "            tf.float32)\n",
    "        # embed_token_ids: [batch_size, num_timesteps-1, num_embedding_nodes]\n",
    "        embed_token_ids = tf.nn.embedding_lookup(embeddings, sentence[:, 0:num_timesteps-1])\n",
    "\n",
    "    img_feature_embed_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('image_feature_embed', initializer=img_feature_embed_init):\n",
    "        # img_feature: [batch_size, img_feature_dim]\n",
    "        # after fc:    [batch_size, num_embedding_nodes] as embed_img\n",
    "        # for concat embed_img and embed_token_ids, \n",
    "        # need to make embed_img & embed_token_id the same size\n",
    "        embed_img = tf.layers.dense(img_feature, hps.num_embedding_nodes)\n",
    "        # now: embed_img: [batch_size, 1, num_embedding_nodes]\n",
    "        embed_img = tf.expand_dims(embed_img, 1)  #add one dimention\n",
    "        # embed_inputs: [batch_size, num_timesteps, num_embedding_nodes]\n",
    "        embed_inputs = tf.concat([embed_img, embed_token_ids], axis=1)\n",
    "        \n",
    "        \"\"\"Now we have our inputs for LSTM\"\"\"\n",
    "\n",
    "        \n",
    "        \n",
    "    # Sets up LSTM network.\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        # 多层的LSTM\n",
    "        cells = []  \n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, keep_prob)  #drop out wrapper\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "        initial_state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        # rnn_outputs: [batch_size, num_timesteps, hps.num_lstm_node[-1]] \n",
    "        rnn_outputs, _ = tf.nn.dynamic_rnn(cell,\n",
    "                                           embed_inputs,\n",
    "                                           initial_state=initial_state)\n",
    "\n",
    "    # Sets up the fully-connected layer.\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        rnn_outputs_2d = tf.reshape(rnn_outputs, [-1, hps.num_lstm_nodes[-1]])\n",
    "        fc1 = tf.layers.dense(rnn_outputs_2d, hps.num_fc_nodes, name='fc1')\n",
    "        fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "        fc1_dropout = tf.nn.relu(fc1_dropout)\n",
    "        logits = tf.layers.dense(fc1_dropout, vocab_size, name='logits')\n",
    "\n",
    "        \n",
    "    # calculate loss\n",
    "    with tf.variable_scope('loss'):\n",
    "        # flatten the ground truth, & mask\n",
    "        sentence_flatten = tf.reshape(sentence, [-1])  \n",
    "        mask_flatten = tf.reshape(mask, [-1])\n",
    "        mask_sum = tf.reduce_sum(mask_flatten)\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, \n",
    "            labels=sentence_flatten)\n",
    "        \n",
    "        #去除weight为0 的元素\n",
    "        weighted_softmax_loss = tf.multiply(softmax_loss,\n",
    "                                            tf.cast(mask_flatten, tf.float32))\n",
    "        \n",
    "        # calculate accuracy\n",
    "        prediction = tf.argmax(logits, 1, output_type = tf.int32)\n",
    "        correct_prediction = tf.equal(prediction, sentence_flatten)\n",
    "        correct_prediction_with_mask = tf.multiply(\n",
    "            tf.cast(correct_prediction, tf.float32),\n",
    "            mask_flatten)\n",
    "        accuracy = tf.reduce_sum(correct_prediction_with_mask) / mask_sum\n",
    "        \n",
    "        loss = tf.reduce_sum(weighted_softmax_loss) / mask_sum\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "        \n",
    "    # define train_op\n",
    "    with tf.variable_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            logging.info(\"variable name: %s\" % (var.name))\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        for grad, var in zip(grads, tvars):\n",
    "            tf.summary.histogram('%s_grad' % (var.name), grad)\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step)\n",
    "\n",
    "    return ((img_feature, sentence, mask, keep_prob),\n",
    "            (loss, accuracy, train_op),\n",
    "            global_step)\n",
    "\n",
    "placeholders, metrics, global_step = get_train_model(hps, vocab_size, img_feature_dim)\n",
    "img_feature, sentence, mask, keep_prob = placeholders\n",
    "loss, accuracy, train_op = metrics\n",
    "\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:    10, loss: 9.243, accuracy: 0.002\n",
      "INFO:tensorflow:Step:    20, loss: 9.128, accuracy: 0.000\n",
      "INFO:tensorflow:Step:    30, loss: 8.866, accuracy: 0.002\n",
      "INFO:tensorflow:Step:    40, loss: 8.410, accuracy: 0.008\n",
      "INFO:tensorflow:Step:    50, loss: 7.825, accuracy: 0.012\n",
      "INFO:tensorflow:Step:    60, loss: 7.263, accuracy: 0.028\n",
      "INFO:tensorflow:Step:    70, loss: 6.833, accuracy: 0.074\n",
      "INFO:tensorflow:Step:    80, loss: 6.207, accuracy: 0.086\n",
      "INFO:tensorflow:Step:    90, loss: 6.125, accuracy: 0.110\n",
      "INFO:tensorflow:Step:   100, loss: 5.882, accuracy: 0.128\n",
      "INFO:tensorflow:Step: 100, image caption model saved\n",
      "INFO:tensorflow:Step:   110, loss: 6.174, accuracy: 0.098\n",
      "INFO:tensorflow:Step:   120, loss: 5.802, accuracy: 0.138\n",
      "INFO:tensorflow:Step:   130, loss: 5.587, accuracy: 0.134\n",
      "INFO:tensorflow:Step:   140, loss: 5.563, accuracy: 0.134\n",
      "INFO:tensorflow:Step:   150, loss: 5.739, accuracy: 0.124\n",
      "INFO:tensorflow:Step:   160, loss: 5.637, accuracy: 0.136\n",
      "INFO:tensorflow:Step:   170, loss: 5.474, accuracy: 0.140\n",
      "INFO:tensorflow:Step:   180, loss: 5.638, accuracy: 0.150\n",
      "INFO:tensorflow:Step:   190, loss: 5.397, accuracy: 0.156\n",
      "INFO:tensorflow:Step:   200, loss: 5.459, accuracy: 0.160\n",
      "INFO:tensorflow:Step: 200, image caption model saved\n",
      "INFO:tensorflow:Step:   210, loss: 5.499, accuracy: 0.144\n",
      "INFO:tensorflow:Step:   220, loss: 5.824, accuracy: 0.128\n",
      "INFO:tensorflow:Step:   230, loss: 5.475, accuracy: 0.148\n",
      "INFO:tensorflow:Step:   240, loss: 5.402, accuracy: 0.166\n",
      "INFO:tensorflow:Step:   250, loss: 5.353, accuracy: 0.170\n",
      "INFO:tensorflow:Step:   260, loss: 5.403, accuracy: 0.171\n",
      "INFO:tensorflow:Step:   270, loss: 5.290, accuracy: 0.166\n",
      "INFO:tensorflow:Step:   280, loss: 5.440, accuracy: 0.166\n",
      "INFO:tensorflow:Step:   290, loss: 5.268, accuracy: 0.174\n",
      "INFO:tensorflow:Step:   300, loss: 5.604, accuracy: 0.126\n",
      "INFO:tensorflow:Step: 300, image caption model saved\n",
      "INFO:tensorflow:Step:   310, loss: 5.517, accuracy: 0.142\n",
      "INFO:tensorflow:Step:   320, loss: 5.153, accuracy: 0.213\n",
      "INFO:tensorflow:Step:   330, loss: 5.395, accuracy: 0.184\n",
      "INFO:tensorflow:Step:   340, loss: 5.317, accuracy: 0.176\n",
      "INFO:tensorflow:Step:   350, loss: 5.367, accuracy: 0.156\n",
      "INFO:tensorflow:Step:   360, loss: 5.080, accuracy: 0.198\n",
      "INFO:tensorflow:Step:   370, loss: 5.214, accuracy: 0.184\n",
      "INFO:tensorflow:Step:   380, loss: 5.153, accuracy: 0.174\n",
      "INFO:tensorflow:Step:   390, loss: 5.296, accuracy: 0.167\n",
      "INFO:tensorflow:Step:   400, loss: 4.884, accuracy: 0.220\n",
      "INFO:tensorflow:Step: 400, image caption model saved\n",
      "INFO:tensorflow:Step:   410, loss: 5.079, accuracy: 0.180\n",
      "INFO:tensorflow:Step:   420, loss: 5.068, accuracy: 0.190\n",
      "INFO:tensorflow:Step:   430, loss: 5.214, accuracy: 0.176\n",
      "INFO:tensorflow:Step:   440, loss: 5.467, accuracy: 0.181\n",
      "INFO:tensorflow:Step:   450, loss: 4.883, accuracy: 0.216\n",
      "INFO:tensorflow:Step:   460, loss: 5.072, accuracy: 0.213\n",
      "INFO:tensorflow:Step:   470, loss: 5.045, accuracy: 0.208\n",
      "INFO:tensorflow:Step:   480, loss: 4.931, accuracy: 0.216\n",
      "INFO:tensorflow:Step:   490, loss: 5.036, accuracy: 0.220\n",
      "INFO:tensorflow:Step:   500, loss: 5.133, accuracy: 0.205\n",
      "INFO:tensorflow:Step: 500, image caption model saved\n",
      "INFO:tensorflow:Step:   510, loss: 5.387, accuracy: 0.193\n",
      "INFO:tensorflow:Step:   520, loss: 5.053, accuracy: 0.194\n",
      "INFO:tensorflow:Step:   530, loss: 4.850, accuracy: 0.238\n",
      "INFO:tensorflow:Step:   540, loss: 5.165, accuracy: 0.188\n",
      "INFO:tensorflow:Step:   550, loss: 4.917, accuracy: 0.198\n",
      "INFO:tensorflow:Step:   560, loss: 4.915, accuracy: 0.184\n",
      "INFO:tensorflow:Step:   570, loss: 4.812, accuracy: 0.246\n",
      "INFO:tensorflow:Step:   580, loss: 4.827, accuracy: 0.218\n",
      "INFO:tensorflow:Step:   590, loss: 4.958, accuracy: 0.192\n",
      "INFO:tensorflow:Step:   600, loss: 4.631, accuracy: 0.230\n",
      "INFO:tensorflow:Step: 600, image caption model saved\n",
      "INFO:tensorflow:Step:   610, loss: 4.663, accuracy: 0.195\n",
      "INFO:tensorflow:Step:   620, loss: 4.787, accuracy: 0.222\n",
      "INFO:tensorflow:Step:   630, loss: 5.056, accuracy: 0.202\n",
      "INFO:tensorflow:Step:   640, loss: 4.947, accuracy: 0.210\n",
      "INFO:tensorflow:Step:   650, loss: 5.102, accuracy: 0.176\n",
      "INFO:tensorflow:Step:   660, loss: 4.451, accuracy: 0.260\n",
      "INFO:tensorflow:Step:   670, loss: 4.517, accuracy: 0.252\n",
      "INFO:tensorflow:Step:   680, loss: 4.770, accuracy: 0.190\n",
      "INFO:tensorflow:Step:   690, loss: 4.979, accuracy: 0.218\n",
      "INFO:tensorflow:Step:   700, loss: 4.483, accuracy: 0.224\n",
      "INFO:tensorflow:Step: 700, image caption model saved\n",
      "INFO:tensorflow:Step:   710, loss: 4.829, accuracy: 0.234\n",
      "INFO:tensorflow:Step:   720, loss: 4.514, accuracy: 0.236\n",
      "INFO:tensorflow:Step:   730, loss: 4.837, accuracy: 0.218\n",
      "INFO:tensorflow:Step:   740, loss: 4.901, accuracy: 0.214\n",
      "INFO:tensorflow:Step:   750, loss: 4.843, accuracy: 0.206\n",
      "INFO:tensorflow:Step:   760, loss: 4.847, accuracy: 0.226\n",
      "INFO:tensorflow:Step:   770, loss: 4.642, accuracy: 0.222\n",
      "INFO:tensorflow:Step:   780, loss: 4.812, accuracy: 0.242\n",
      "INFO:tensorflow:Step:   790, loss: 4.694, accuracy: 0.226\n",
      "INFO:tensorflow:Step:   800, loss: 4.412, accuracy: 0.272\n",
      "INFO:tensorflow:Step: 800, image caption model saved\n",
      "INFO:tensorflow:Step:   810, loss: 4.703, accuracy: 0.236\n",
      "INFO:tensorflow:Step:   820, loss: 4.809, accuracy: 0.220\n",
      "INFO:tensorflow:Step:   830, loss: 4.496, accuracy: 0.228\n",
      "INFO:tensorflow:Step:   840, loss: 4.666, accuracy: 0.228\n",
      "INFO:tensorflow:Step:   850, loss: 4.874, accuracy: 0.222\n",
      "INFO:tensorflow:Step:   860, loss: 4.830, accuracy: 0.228\n",
      "INFO:tensorflow:Step:   870, loss: 4.416, accuracy: 0.250\n",
      "INFO:tensorflow:Step:   880, loss: 4.668, accuracy: 0.238\n",
      "INFO:tensorflow:Step:   890, loss: 4.500, accuracy: 0.242\n",
      "INFO:tensorflow:Step:   900, loss: 4.839, accuracy: 0.218\n",
      "INFO:tensorflow:Step: 900, image caption model saved\n",
      "INFO:tensorflow:Step:   910, loss: 4.551, accuracy: 0.236\n",
      "INFO:tensorflow:Step:   920, loss: 4.456, accuracy: 0.246\n",
      "INFO:tensorflow:Step:   930, loss: 4.594, accuracy: 0.234\n",
      "INFO:tensorflow:Step:   940, loss: 4.697, accuracy: 0.212\n",
      "INFO:tensorflow:Step:   950, loss: 4.785, accuracy: 0.216\n",
      "INFO:tensorflow:Step:   960, loss: 4.503, accuracy: 0.246\n",
      "INFO:tensorflow:Step:   970, loss: 4.415, accuracy: 0.234\n",
      "INFO:tensorflow:Step:   980, loss: 4.685, accuracy: 0.224\n",
      "INFO:tensorflow:Step:   990, loss: 4.401, accuracy: 0.224\n",
      "INFO:tensorflow:Step:  1000, loss: 4.667, accuracy: 0.226\n",
      "INFO:tensorflow:Step: 1000, image caption model saved\n",
      "INFO:tensorflow:Step:  1010, loss: 4.750, accuracy: 0.222\n",
      "INFO:tensorflow:Step:  1020, loss: 4.315, accuracy: 0.230\n",
      "INFO:tensorflow:Step:  1030, loss: 4.375, accuracy: 0.246\n",
      "INFO:tensorflow:Step:  1040, loss: 4.637, accuracy: 0.250\n",
      "INFO:tensorflow:Step:  1050, loss: 4.615, accuracy: 0.202\n",
      "INFO:tensorflow:Step:  1060, loss: 4.580, accuracy: 0.238\n",
      "INFO:tensorflow:Step:  1070, loss: 5.030, accuracy: 0.188\n",
      "INFO:tensorflow:Step:  1080, loss: 4.325, accuracy: 0.254\n",
      "INFO:tensorflow:Step:  1090, loss: 4.690, accuracy: 0.214\n",
      "INFO:tensorflow:Step:  1100, loss: 4.416, accuracy: 0.262\n",
      "INFO:tensorflow:Step: 1100, image caption model saved\n",
      "INFO:tensorflow:Step:  1110, loss: 4.345, accuracy: 0.270\n",
      "INFO:tensorflow:Step:  1120, loss: 4.474, accuracy: 0.256\n",
      "INFO:tensorflow:Step:  1130, loss: 4.490, accuracy: 0.262\n",
      "INFO:tensorflow:Step:  1140, loss: 4.624, accuracy: 0.266\n",
      "INFO:tensorflow:Step:  1150, loss: 4.245, accuracy: 0.258\n",
      "INFO:tensorflow:Step:  1160, loss: 4.660, accuracy: 0.260\n",
      "INFO:tensorflow:Step:  1170, loss: 4.583, accuracy: 0.204\n",
      "INFO:tensorflow:Step:  1180, loss: 4.417, accuracy: 0.258\n",
      "INFO:tensorflow:Step:  1190, loss: 4.450, accuracy: 0.238\n",
      "INFO:tensorflow:Step:  1200, loss: 4.766, accuracy: 0.212\n",
      "INFO:tensorflow:Step: 1200, image caption model saved\n",
      "INFO:tensorflow:Step:  1210, loss: 4.377, accuracy: 0.244\n",
      "INFO:tensorflow:Step:  1220, loss: 4.039, accuracy: 0.292\n",
      "INFO:tensorflow:Step:  1230, loss: 4.647, accuracy: 0.232\n",
      "INFO:tensorflow:Step:  1240, loss: 4.239, accuracy: 0.256\n",
      "INFO:tensorflow:Step:  1250, loss: 4.603, accuracy: 0.232\n",
      "INFO:tensorflow:Step:  1260, loss: 4.427, accuracy: 0.264\n",
      "INFO:tensorflow:Step:  1270, loss: 4.674, accuracy: 0.240\n",
      "INFO:tensorflow:Step:  1280, loss: 4.069, accuracy: 0.268\n",
      "INFO:tensorflow:Step:  1290, loss: 4.394, accuracy: 0.254\n",
      "INFO:tensorflow:Step:  1300, loss: 4.409, accuracy: 0.280\n",
      "INFO:tensorflow:Step: 1300, image caption model saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:  1310, loss: 4.521, accuracy: 0.252\n",
      "INFO:tensorflow:Step:  1320, loss: 4.124, accuracy: 0.282\n",
      "INFO:tensorflow:Step:  1330, loss: 4.174, accuracy: 0.264\n",
      "INFO:tensorflow:Step:  1340, loss: 4.279, accuracy: 0.262\n",
      "INFO:tensorflow:Step:  1350, loss: 4.667, accuracy: 0.244\n",
      "INFO:tensorflow:Step:  1360, loss: 4.277, accuracy: 0.260\n",
      "INFO:tensorflow:Step:  1370, loss: 4.249, accuracy: 0.246\n",
      "INFO:tensorflow:Step:  1380, loss: 4.273, accuracy: 0.280\n",
      "INFO:tensorflow:Step:  1390, loss: 4.661, accuracy: 0.240\n",
      "INFO:tensorflow:Step:  1400, loss: 4.558, accuracy: 0.240\n",
      "INFO:tensorflow:Step: 1400, image caption model saved\n",
      "INFO:tensorflow:Step:  1410, loss: 4.214, accuracy: 0.270\n",
      "INFO:tensorflow:Step:  1420, loss: 4.283, accuracy: 0.280\n",
      "INFO:tensorflow:Step:  1430, loss: 4.368, accuracy: 0.266\n",
      "INFO:tensorflow:Step:  1440, loss: 4.470, accuracy: 0.254\n",
      "INFO:tensorflow:Step:  1450, loss: 4.201, accuracy: 0.250\n",
      "INFO:tensorflow:Step:  1460, loss: 4.298, accuracy: 0.268\n",
      "INFO:tensorflow:Step:  1470, loss: 4.260, accuracy: 0.288\n",
      "INFO:tensorflow:Step:  1480, loss: 4.244, accuracy: 0.250\n",
      "INFO:tensorflow:Step:  1490, loss: 4.079, accuracy: 0.264\n",
      "INFO:tensorflow:Step:  1500, loss: 4.427, accuracy: 0.260\n",
      "INFO:tensorflow:Step: 1500, image caption model saved\n",
      "INFO:tensorflow:Step:  1510, loss: 4.184, accuracy: 0.294\n",
      "INFO:tensorflow:Step:  1520, loss: 4.233, accuracy: 0.252\n",
      "INFO:tensorflow:Step:  1530, loss: 4.504, accuracy: 0.218\n",
      "INFO:tensorflow:Step:  1540, loss: 4.278, accuracy: 0.264\n",
      "INFO:tensorflow:Step:  1550, loss: 4.069, accuracy: 0.294\n",
      "INFO:tensorflow:Step:  1560, loss: 4.606, accuracy: 0.236\n",
      "INFO:tensorflow:Step:  1570, loss: 4.510, accuracy: 0.244\n",
      "INFO:tensorflow:Step:  1580, loss: 4.151, accuracy: 0.268\n",
      "INFO:tensorflow:Step:  1590, loss: 4.545, accuracy: 0.228\n",
      "INFO:tensorflow:Step:  1600, loss: 4.435, accuracy: 0.222\n",
      "INFO:tensorflow:Step: 1600, image caption model saved\n",
      "INFO:tensorflow:Step:  1610, loss: 4.407, accuracy: 0.254\n",
      "INFO:tensorflow:Step:  1620, loss: 4.561, accuracy: 0.222\n",
      "INFO:tensorflow:Step:  1630, loss: 4.301, accuracy: 0.274\n",
      "INFO:tensorflow:Step:  1640, loss: 4.419, accuracy: 0.248\n",
      "INFO:tensorflow:Step:  1650, loss: 4.412, accuracy: 0.264\n",
      "INFO:tensorflow:Step:  1660, loss: 4.290, accuracy: 0.250\n",
      "INFO:tensorflow:Step:  1670, loss: 4.531, accuracy: 0.226\n",
      "INFO:tensorflow:Step:  1680, loss: 4.075, accuracy: 0.266\n",
      "INFO:tensorflow:Step:  1690, loss: 4.249, accuracy: 0.274\n",
      "INFO:tensorflow:Step:  1700, loss: 4.321, accuracy: 0.234\n",
      "INFO:tensorflow:Step: 1700, image caption model saved\n",
      "INFO:tensorflow:Step:  1710, loss: 4.171, accuracy: 0.262\n",
      "INFO:tensorflow:Step:  1720, loss: 4.357, accuracy: 0.250\n",
      "INFO:tensorflow:Step:  1730, loss: 4.587, accuracy: 0.262\n",
      "INFO:tensorflow:Step:  1740, loss: 4.425, accuracy: 0.246\n",
      "INFO:tensorflow:Step:  1750, loss: 4.315, accuracy: 0.272\n",
      "INFO:tensorflow:Step:  1760, loss: 4.392, accuracy: 0.272\n",
      "INFO:tensorflow:Step:  1770, loss: 4.249, accuracy: 0.252\n",
      "INFO:tensorflow:Step:  1780, loss: 4.268, accuracy: 0.264\n",
      "INFO:tensorflow:Step:  1790, loss: 4.391, accuracy: 0.252\n",
      "INFO:tensorflow:Step:  1800, loss: 4.298, accuracy: 0.250\n",
      "INFO:tensorflow:Step: 1800, image caption model saved\n",
      "INFO:tensorflow:Step:  1810, loss: 4.366, accuracy: 0.246\n",
      "INFO:tensorflow:Step:  1820, loss: 4.103, accuracy: 0.282\n",
      "INFO:tensorflow:Step:  1830, loss: 3.862, accuracy: 0.334\n",
      "INFO:tensorflow:Step:  1840, loss: 4.107, accuracy: 0.276\n",
      "INFO:tensorflow:Step:  1850, loss: 4.235, accuracy: 0.258\n",
      "INFO:tensorflow:Step:  1860, loss: 4.284, accuracy: 0.264\n",
      "INFO:tensorflow:Step:  1870, loss: 4.198, accuracy: 0.252\n",
      "INFO:tensorflow:Step:  1880, loss: 4.286, accuracy: 0.276\n",
      "INFO:tensorflow:Step:  1890, loss: 4.344, accuracy: 0.246\n",
      "INFO:tensorflow:Step:  1900, loss: 4.181, accuracy: 0.250\n",
      "INFO:tensorflow:Step: 1900, image caption model saved\n",
      "INFO:tensorflow:Step:  1910, loss: 4.345, accuracy: 0.250\n",
      "INFO:tensorflow:Step:  1920, loss: 4.453, accuracy: 0.250\n",
      "INFO:tensorflow:Step:  1930, loss: 4.265, accuracy: 0.268\n",
      "INFO:tensorflow:Step:  1940, loss: 4.098, accuracy: 0.266\n",
      "INFO:tensorflow:Step:  1950, loss: 4.069, accuracy: 0.278\n",
      "INFO:tensorflow:Step:  1960, loss: 4.071, accuracy: 0.252\n",
      "INFO:tensorflow:Step:  1970, loss: 4.136, accuracy: 0.268\n",
      "INFO:tensorflow:Step:  1980, loss: 4.333, accuracy: 0.266\n",
      "INFO:tensorflow:Step:  1990, loss: 4.233, accuracy: 0.274\n",
      "INFO:tensorflow:Step:  2000, loss: 4.250, accuracy: 0.254\n",
      "INFO:tensorflow:Step: 2000, image caption model saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "training run this graph\n",
    "\"\"\"\n",
    "training_steps = 2000  # 10000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    writer = tf.summary.FileWriter(output_dir, sess.graph)\n",
    "    for i in range(training_steps):\n",
    "        (batch_img_features, \n",
    "         batch_sentence_ids, \n",
    "         batch_weights, _) = caption_data.next_batch(hps.batch_size)\n",
    "        input_vals = (batch_img_features, \n",
    "                      batch_sentence_ids, \n",
    "                      batch_weights, \n",
    "                      hps.keep_prob)\n",
    "        \n",
    "        feed_dict = dict(zip(placeholders, input_vals))\n",
    "        fetches = [global_step, loss, accuracy, train_op]\n",
    "        \n",
    "        should_log = (i + 1) % hps.log_frequent == 0\n",
    "        should_save = (i + 1) % hps.save_frequent == 0\n",
    "        if should_log:\n",
    "            fetches += [summary_op]\n",
    "        outputs = sess.run(fetches, feed_dict)\n",
    "        global_step_val, loss_val, accuracy_val = outputs[0:3]\n",
    "        \n",
    "        if should_log:\n",
    "            summary_str = outputs[4]\n",
    "            writer.add_summary(summary_str, global_step_val)\n",
    "            logging.info('Step: %5d, loss: %3.3f, accuracy: %3.3f'\n",
    "                         % (global_step_val, loss_val, accuracy_val))\n",
    "        if should_save:\n",
    "            logging.info(\"Step: %d, image caption model saved\" % (global_step_val))\n",
    "            saver.save(sess, os.path.join(output_dir, \"image_caption\"), global_step=global_step_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
